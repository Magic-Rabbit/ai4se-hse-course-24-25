{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "709341da8b614405aa8c03a260edbc34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d739db6fe0d4cc6b25865b0f9ab27b7",
              "IPY_MODEL_7b708facc80b4d15a871b1a8e6e31a91",
              "IPY_MODEL_7f93081a8f4d4f95b09a88a29bc122b7"
            ],
            "layout": "IPY_MODEL_f829690b22cb43e5b69a6ee5160c4f3d"
          }
        },
        "9d739db6fe0d4cc6b25865b0f9ab27b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78b35c90a89d445081a3fa7bb56ec670",
            "placeholder": "​",
            "style": "IPY_MODEL_09fcb8356a5e4a6c83cb7f56c80b57c5",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "7b708facc80b4d15a871b1a8e6e31a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea68f33e59ab4f4caed05bf8f1b0174f",
            "max": 12904,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af32612bb95b4c07bb13a17e5ad87756",
            "value": 12904
          }
        },
        "7f93081a8f4d4f95b09a88a29bc122b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1ddc3a4e0f84f26afa67f35da8b9bfd",
            "placeholder": "​",
            "style": "IPY_MODEL_014be6400d32499892ac94f0e73eeee8",
            "value": " 12904/12904 [00:00&lt;00:00, 214097.87 examples/s]"
          }
        },
        "f829690b22cb43e5b69a6ee5160c4f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78b35c90a89d445081a3fa7bb56ec670": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09fcb8356a5e4a6c83cb7f56c80b57c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea68f33e59ab4f4caed05bf8f1b0174f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af32612bb95b4c07bb13a17e5ad87756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1ddc3a4e0f84f26afa67f35da8b9bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "014be6400d32499892ac94f0e73eeee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ai4se-course/ai4se-hse-course-24-25.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv8dMDJVEmlh",
        "outputId": "c1552139-088d-417a-fac7-76a017381d91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai4se-hse-course-24-25'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 12 (delta 0), reused 0 (delta 0), pack-reused 5 (from 1)\u001b[K\n",
            "Receiving objects: 100% (12/12), 4.42 KiB | 4.42 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r \"ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt\"\n",
        "!pip install -r \"ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TkQ_zzK3Et7H",
        "outputId": "db18da08-10d1-44ae-f17a-32f27c283a2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.20.0 (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1))\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate==0.4.2 (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 2))\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 3)) (1.26.4)\n",
            "Collecting scikit-learn==1.5.1 (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 4))\n",
            "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting tqdm==4.66.4 (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 5))\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (2.32.3)\n",
            "Collecting xxhash (from datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1))\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.1->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.1->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.1->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (2024.8.30)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==2.20.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements.txt (line 1)) (0.2.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, tqdm, fsspec, dill, scikit-learn, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.5.2\n",
            "    Uninstalling scikit-learn-1.5.2:\n",
            "      Successfully uninstalled scikit-learn-1.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 evaluate-0.4.2 fsspec-2024.5.0 multiprocess-0.70.16 scikit-learn-1.5.1 tqdm-4.66.4 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              },
              "id": "cab0eb96e99a4146a1f5d62ec07697f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting black==22.3.0 (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 1))\n",
            "  Downloading black-22.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (45 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flake8==4.0.1 (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 2))\n",
            "  Downloading flake8-4.0.1-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting flake8-builtins==1.5.3 (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 3))\n",
            "  Downloading flake8_builtins-1.5.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting flake8-comprehensions==3.10.0 (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 4))\n",
            "  Downloading flake8_comprehensions-3.10.0-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting flake8-docstrings==1.5.0 (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 5))\n",
            "  Downloading flake8_docstrings-1.5.0-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting flake8-import-order==0.18.1 (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 6))\n",
            "  Downloading flake8_import_order-0.18.1-py2.py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting flake8-pep585==0.1.5.1 (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 7))\n",
            "  Downloading flake8_pep585-0.1.5.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting flake8-quotes==2.1.1 (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 8))\n",
            "  Downloading flake8-quotes-2.1.1.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flakeheaven==3.0.0 (from -r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 9))\n",
            "  Downloading flakeheaven-3.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black==22.3.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black==22.3.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 1)) (4.3.6)\n",
            "Collecting pathspec>=0.9.0 (from black==22.3.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 1))\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting mypy-extensions>=0.4.3 (from black==22.3.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 1))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black==22.3.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 1)) (2.0.2)\n",
            "Collecting mccabe<0.7.0,>=0.6.0 (from flake8==4.0.1->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 2))\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pycodestyle<2.9.0,>=2.8.0 (from flake8==4.0.1->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 2))\n",
            "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl.metadata (31 kB)\n",
            "Collecting pyflakes<2.5.0,>=2.4.0 (from flake8==4.0.1->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 2))\n",
            "  Downloading pyflakes-2.4.0-py2.py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pydocstyle>=2.1 (from flake8-docstrings==1.5.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 5))\n",
            "  Downloading pydocstyle-6.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from flake8-import-order==0.18.1->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 6)) (75.1.0)\n",
            "Collecting colorama (from flakeheaven==3.0.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 9))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from flakeheaven==3.0.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 9)) (0.4)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from flakeheaven==3.0.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 9)) (2.18.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from flakeheaven==3.0.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 9)) (0.10.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from flakeheaven==3.0.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 9)) (2.2.3)\n",
            "Requirement already satisfied: snowballstemmer>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pydocstyle>=2.1->flake8-docstrings==1.5.0->-r ai4se-hse-course-24-25/01-toxic-review-classification/requirements_dev.txt (line 5)) (2.2.0)\n",
            "Downloading black-22.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flake8-4.0.1-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flake8_builtins-1.5.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flake8_comprehensions-3.10.0-py3-none-any.whl (7.3 kB)\n",
            "Downloading flake8_docstrings-1.5.0-py2.py3-none-any.whl (5.5 kB)\n",
            "Downloading flake8_import_order-0.18.1-py2.py3-none-any.whl (15 kB)\n",
            "Downloading flake8_pep585-0.1.5.1-py3-none-any.whl (9.8 kB)\n",
            "Downloading flakeheaven-3.0.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydocstyle-6.3.0-py3-none-any.whl (38 kB)\n",
            "Downloading pyflakes-2.4.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Building wheels for collected packages: flake8-quotes\n",
            "  Building wheel for flake8-quotes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flake8-quotes: filename=flake8_quotes-2.1.1-py3-none-any.whl size=8376 sha256=e2a75003e4c1c0901948fd255410193fabef44f921eb4d5d4fc39580d11d0863\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/d2/2d/591df977f2124f527a2ed9fd9a127191516666f3be22b0a2f7\n",
            "Successfully built flake8-quotes\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LgtCTg3MD4qt"
      },
      "outputs": [],
      "source": [
        "from statistics import mean, stdev\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "def classifier(dataset, model):\n",
        "    X = dataset['message']\n",
        "    y = dataset['is_toxic']\n",
        "\n",
        "    if model == 'classic_ml':\n",
        "        # Преобразование текста в числовое представление\n",
        "\n",
        "        vectorizers = {\n",
        "            'tfidf': TfidfVectorizer(),\n",
        "            'count': CountVectorizer()\n",
        "        }\n",
        "\n",
        "        models = {\n",
        "            'rf': RandomForestClassifier(),\n",
        "            'lr': LogisticRegression()\n",
        "        }\n",
        "\n",
        "        for vect_name, vectorizer in vectorizers.items():\n",
        "          X_vec = vectorizer.fit_transform(X)\n",
        "          for model_name, model_instance in models.items():\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2)\n",
        "              model_instance.fit(X_train, y_train)\n",
        "\n",
        "              kf = KFold(n_splits=10, shuffle=True)\n",
        "              scores = cross_val_score(model_instance, X_train, y_train, cv=kf, scoring='f1')\n",
        "              print(f\"{vect_name} - {model_name} - f1: {scores.mean():.4f}\")\n",
        "\n",
        "\n",
        "              y_pred = model_instance.predict(X_test)\n",
        "              print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    elif model == 'microsoft/codebert-base':\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model, num_labels=2)\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "\n",
        "        class Dataset(torch.utils.data.Dataset):\n",
        "            def __init__(self, encodings, labels):\n",
        "                self.encodings = encodings\n",
        "                self.labels = labels\n",
        "\n",
        "            def __getitem__(self, idx):\n",
        "                item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n",
        "                item['labels'] = torch.tensor(self.labels[idx]).to(device)\n",
        "                return item\n",
        "\n",
        "            def __len__(self):\n",
        "                return len(self.labels)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        train_encodings = tokenizer(X_train, padding=True, truncation=True, return_tensors='pt')\n",
        "        test_encodings = tokenizer(X_test, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "        train_dataset = Dataset(train_encodings, y_train)\n",
        "        test_dataset = Dataset(test_encodings, y_test)\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir='./results',\n",
        "            num_train_epochs=3,\n",
        "            dataloader_pin_memory=False,\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            warmup_steps=500,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir='./logs',\n",
        "            evaluation_strategy=\"epoch\"\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "            compute_metrics=compute_metrics\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        codebert_eval_results = trainer.evaluate()\n",
        "        print(codebert_eval_results)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import re\n",
        "import datasets\n",
        "import pandas as pd\n",
        "\n",
        "contractions_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\n",
        "                       \"can't\": \"cannot\", \"'cause\": \"because\",\n",
        "                       \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                       \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
        "                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
        "                       \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
        "                       \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\",\n",
        "                       \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\",\n",
        "                       \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
        "                       \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "                       \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n",
        "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                       \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
        "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
        "                       \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
        "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
        "                       \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
        "                       \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                       \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
        "                       \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                       \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
        "                       \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n",
        "                       \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
        "                       \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                       \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
        "                       \"here's\": \"here is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                       \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
        "                       \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
        "                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
        "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
        "                       \"what'll\": \"what will\",\n",
        "                       \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
        "                       \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n",
        "                       \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n",
        "                       \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\",\n",
        "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
        "                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
        "                       \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
        "                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                       \"you're\": \"you are\", \"you've\": \"you have\", \"aint\": \"is not\", \"arent\": \"are not\",\n",
        "                       \"cant\": \"cannot\", \"cause\": \"because\",\n",
        "                       \"couldve\": \"could have\", \"couldnt\": \"could not\",\n",
        "                       \"didnt\": \"did not\", \"doesnt\": \"does not\",\n",
        "                       \"dont\": \"do not\", \"hadnt\": \"had not\", \"hasnt\": \"has not\",\n",
        "                       \"havent\": \"have not\", \"howdy\": \"how do you\",\n",
        "                       \"its\": \"it is\", \"lets\": \"let us\", \"maam\": \"madam\", \"maynt\": \"may not\",\n",
        "                       \"mightve\": \"might have\", \"mightnt\": \"might not\",\n",
        "                       \"mightntve\": \"might not have\", \"mustve\": \"must have\",\n",
        "                       \"mustnt\": \"must not\", \"mustntve\": \"must not have\",\n",
        "                       \"neednt\": \"need not\", \"needntve\": \"need not have\",\n",
        "                       \"oclock\": \"of the clock\", \"oughtnt\": \"ought not\",\n",
        "                       \"shouldve\": \"should have\", \"shouldnt\": \"should not\",\n",
        "                       \"werent\": \"were not\", \"yall\": \"you all\", \"youre\": \"you are\",\n",
        "                       \"youve\": \"you have\"}\n",
        "\n",
        "profanity_dict = {\n",
        "    r'(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*': 'fuck',\n",
        "    r'(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)': 'fuck',\n",
        "    r' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k': 'fuck',\n",
        "    r'f u u c': 'fuck',\n",
        "    r'(f)(c|[^a-z ])(u|[^a-z ])(k)': 'fuck',\n",
        "    r'f\\*': 'fuck',\n",
        "    r'feck ': 'fuck',\n",
        "    r' fux ': 'fuck',\n",
        "    r'f\\*\\*': 'fuck',\n",
        "    r'f\\-ing': 'fuck',\n",
        "    r'f\\.u\\.': 'fuck',\n",
        "    r'f###': 'fuck',\n",
        "    r' fu ': 'fuck',\n",
        "    r'f@ck': 'fuck',\n",
        "    r'f u c k': 'fuck',\n",
        "    r'f uck': 'fuck',\n",
        "    r'f ck': 'fuck',\n",
        "    r' (c)(r|[^a-z0-9 ])(a|[^a-z0-9 ])(p|[^a-z0-9 ])([^ ])*': 'crap',\n",
        "    r' (c)([^a-z]*)(r)([^a-z]*)(a)([^a-z]*)(p)': 'crap',\n",
        "    r' c[!@#\\$%\\^\\&\\*]*r[!@#\\$%\\^&\\*]*p': 'crap',\n",
        "    r'cr@p': 'crap',\n",
        "    r' c r a p': 'crap',\n",
        "    r'[^a-z]ass ': 'ass',\n",
        "    r'[^a-z]azz ': 'ass',\n",
        "    r'arrse': 'ass',\n",
        "    r' arse ': 'ass',\n",
        "    r'@\\\\$\\\\$': 'ass',\n",
        "    r'[^a-z]anus': 'ass',\n",
        "    r' a\\*s\\*s': 'ass',\n",
        "    r'[^a-z]ass[^a-z ]': 'ass',\n",
        "    r'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]': 'ass',\n",
        "    r'[^a-z]anal ': 'ass',\n",
        "    r'a s s': 'ass',\n",
        "    r' a[s|z]*wipe': 'asshole',\n",
        "    r'a[s|z]*[w]*h[o|0]+[l]*e': 'asshole',\n",
        "    r'@\\\\$\\\\$hole': 'asshole',\n",
        "    r'bitches': 'bitch',\n",
        "    r' b[w]*i[t]*ch': 'bitch',\n",
        "    r' b!tch': 'bitch',\n",
        "    r' bi\\+ch': 'bitch',\n",
        "    r' b!\\+ch': 'bitch',\n",
        "    r' (b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)': 'bitch',\n",
        "    r' biatch': 'bitch',\n",
        "    r' bi\\*\\*h': 'bitch',\n",
        "    r' bytch': 'bitch',\n",
        "    r'b i t c h': 'bitch',\n",
        "    r'ba[s|z]+t[e|a]+rd': 'bastard',\n",
        "    r'transgender': 'transgender',\n",
        "    r'gay': 'gay',\n",
        "    r'homo': 'gay',\n",
        "    r'[^a-z]cock': 'cock',\n",
        "    r'c0ck': 'cock',\n",
        "    r'[^a-z]cok ': 'cock',\n",
        "    r'c0k': 'cock',\n",
        "    r'[^a-z]cok[^aeiou]': 'cock',\n",
        "    r' cawk': 'cock',\n",
        "    r'(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)': 'cock',\n",
        "    r'c o c k': 'cock',\n",
        "    r' dick[^aeiou]': 'dick',\n",
        "    r'd i c k': 'dick',\n",
        "    r'sucker': 'suck',\n",
        "    r'(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)': 'suck',\n",
        "    r'sucks': 'suck',\n",
        "    r'5uck': 'suck',\n",
        "    r's u c k': 'suck',\n",
        "    r'cunt': 'cunt',\n",
        "    r'c u n t': 'cunt',\n",
        "    r'bullsh\\*t': 'bullshit',\n",
        "    r'bull\\\\$hit': 'bullshit',\n",
        "    r'bull sh.t': 'bullshit',\n",
        "    r'jerk': 'jerk',\n",
        "    r'i[d]+io[t]+': 'idiot',\n",
        "    r'(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)': 'idiot',\n",
        "    r'idiots': 'idiot',\n",
        "    r'i d i o t': 'idiot',\n",
        "    r'(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)': 'dumb',\n",
        "    r'shitty': 'shit',\n",
        "    r'(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)': 'shit',\n",
        "    r'shite': 'shit',\n",
        "    r'\\\\$hit': 'shit',\n",
        "    r's h i t': 'shit',\n",
        "    r'sh\\*tty': 'shit',\n",
        "    r'sh\\*ty': 'shit',\n",
        "    r'sh\\*t': 'shit',\n",
        "    r'shythole': 'shit hole',\n",
        "    r'sh.thole': 'shit hole',\n",
        "    r'returd': 'retard',\n",
        "    r'retad': 'retard',\n",
        "    r'retard': 'retard',\n",
        "    r'wiktard': 'retard',\n",
        "    r'wikitud': 'retard',\n",
        "    r'raped': 'rape',\n",
        "    r'dumbass': 'dumb ass',\n",
        "    r'dubass': 'dumb ass',\n",
        "    r'butthead': 'ass head',\n",
        "    r'sexy': 'sex',\n",
        "    r's3x': 'sex',\n",
        "    r'sexuality': 'sex',\n",
        "    r'nigger': 'nigger',\n",
        "    r'ni[g]+a': 'nigger',\n",
        "    r' nigr ': 'nigger',\n",
        "    r'negrito': 'nigger',\n",
        "    r'niguh': 'nigger',\n",
        "    r'n3gr': 'nigger',\n",
        "    r'n i g g e r': 'nigger',\n",
        "    r' stfu': 'shut the fuck up',\n",
        "    r'^stfu': 'shut the fuck up',\n",
        "    r' fyfi': 'for your fucking information',\n",
        "    r'^fyfi': 'for your fucking information',\n",
        "    r'gtfo': 'get the fuck off',\n",
        "    r'^gtfo': 'get the fuck off',\n",
        "    r' omfg': 'oh my fucking god',\n",
        "    r'^omfg': 'oh my fucking god',\n",
        "    r' wth': 'what the hell',\n",
        "    r'^wth': 'what the hell',\n",
        "    r' wtf': 'what the fuck',\n",
        "    r'^wtf': 'what the fuck',\n",
        "    r' sob ': 'son of bitch',\n",
        "    r'^sob ': 'son of bitch',\n",
        "    r'pussy[^c]': 'pussy',\n",
        "    r'pusy': 'pussy',\n",
        "    r'pussi[^l]': 'pussy',\n",
        "    r'pusses': 'pussy',\n",
        "    r'(p)(u|[^a-z0-9 ])(s|[^a-z0-9 ])(s|[^a-z0-9 ])(y)': 'pussy',\n",
        "    r'faggot': 'faggot',\n",
        "    r' fa[g]+[s]*[^a-z ]': 'faggot',\n",
        "    r'fagot': 'faggot',\n",
        "    r'f a g g o t': 'faggot',\n",
        "    r'faggit': 'faggot',\n",
        "    r'(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)': 'faggot',\n",
        "    r'fau[g]+ot': 'faggot',\n",
        "    r'fae[g]+ot': 'faggot',\n",
        "    r' motha f': 'mother fucker',\n",
        "    r' mother f': 'mother fucker',\n",
        "    r'motherucker': 'mother fucker',\n",
        "    r' mofo': 'mother fucker',\n",
        "    r' mf ': 'mother fucker',\n",
        "    r'wh\\*\\*\\*': 'whore',\n",
        "    r'w h o r e': 'whore',\n",
        "    r'ha\\*\\*\\*ha': 'haha',\n",
        "}\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # Удаление URL-ссылок\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Исправление сокращений\n",
        "    for word, correction in contractions_dict.items():\n",
        "        text = re.sub(r'\\b' + word + r'\\b', correction, text)\n",
        "\n",
        "    # Удаление повторяющихся символов\n",
        "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
        "\n",
        "    # Удаление специальных символов\n",
        "    text = re.sub(r'[&^#*]', '', text)\n",
        "\n",
        "    # Исправление ругательных слов\n",
        "    #for pattern, replacement in profanity_dict.items():\n",
        "    #    text = re.sub(r'\\b' + pattern + r'\\b', replacement, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def prepare(raw_data: Path) -> datasets.Dataset:\n",
        "    # Загрузка данных\n",
        "    df = pd.read_excel(raw_data)\n",
        "\n",
        "    # Удаление пропущенных значений и дубликатов\n",
        "    df.dropna(inplace=True)\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Очистка текста\n",
        "    df['message'] = df['message'].apply(clean_text)\n",
        "\n",
        "    # Преобразование DataFrame в Dataset\n",
        "    dataset = datasets.Dataset.from_pandas(df)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def load_dataset(path: Path) -> datasets.Dataset:\n",
        "    return datasets.load_from_disk(str(path))\n",
        "\n",
        "def save_dataset(dataset: datasets.Dataset, path: Path) -> None:\n",
        "    dataset.save_to_disk(str(path))\n",
        "\n"
      ],
      "metadata": {
        "id": "dCjiCJ4DD851"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_data_path = Path('./prepared-dataset')\n",
        "choices=['classic_ml', 'microsoft/codebert-base'],\n",
        "\n",
        "def prepare_data():\n",
        "    dataset = prepare(Path('/content/code-review-dataset-full.xlsx'))\n",
        "    save_dataset(dataset, default_data_path)\n",
        "\n",
        "\n",
        "def classify(model):\n",
        "    dataset = load_dataset(default_data_path)\n",
        "    classifier(dataset, model)"
      ],
      "metadata": {
        "id": "3sWVr5d1D9g9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "709341da8b614405aa8c03a260edbc34",
            "9d739db6fe0d4cc6b25865b0f9ab27b7",
            "7b708facc80b4d15a871b1a8e6e31a91",
            "7f93081a8f4d4f95b09a88a29bc122b7",
            "f829690b22cb43e5b69a6ee5160c4f3d",
            "78b35c90a89d445081a3fa7bb56ec670",
            "09fcb8356a5e4a6c83cb7f56c80b57c5",
            "ea68f33e59ab4f4caed05bf8f1b0174f",
            "af32612bb95b4c07bb13a17e5ad87756",
            "a1ddc3a4e0f84f26afa67f35da8b9bfd",
            "014be6400d32499892ac94f0e73eeee8"
          ]
        },
        "id": "fwf5bthXGckW",
        "outputId": "fe68ed6b-d65f-4566-cb4e-263e39073156"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/12904 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "709341da8b614405aa8c03a260edbc34"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify('classic_ml')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hiVE_uhGlgm",
        "outputId": "822f15d0-25ae-4fe8-b387-432ffa521d8e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf - rf - f1: 0.6575\n",
            "[[2042   32]\n",
            " [ 265  242]]\n",
            "tfidf - lr - f1: 0.5551\n",
            "[[2051   22]\n",
            " [ 304  204]]\n",
            "count - rf - f1: 0.6674\n",
            "[[2035   32]\n",
            " [ 263  251]]\n",
            "count - lr - f1: 0.7170\n",
            "[[2021   55]\n",
            " [ 194  311]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "\n",
        "\n",
        "def experiments_lr(dataset):\n",
        "    X = dataset['message']\n",
        "    y = dataset['is_toxic']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    kf = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', LogisticRegression())\n",
        "    ])\n",
        "\n",
        "    param_grid = {\n",
        "        'tfidf__max_features': [1000, 5000, 10000],\n",
        "        'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "        'clf__C': [0.1, 1, 10],\n",
        "        'clf__solver': ['liblinear', 'lbfgs']\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=kf, scoring='f1')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best score: {grid_search.best_score_}\")\n",
        "\n",
        "def experiments_rf(dataset):\n",
        "    X = dataset['message']\n",
        "    y = dataset['is_toxic']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    kf = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "    param_grid = {\n",
        "        'tfidf__max_features': [1000, 5000, 10000],\n",
        "        'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "        'clf__n_estimators': [100, 200, 300],\n",
        "        'clf__max_depth': [None, 10, 20],\n",
        "        'clf__min_samples_split': [2, 5, 10],\n",
        "        'clf__min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=kf, scoring='f1')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best score: {grid_search.best_score_}\")\n",
        "\n",
        "experiments_lr(load_dataset(default_data_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fKrp64g7bzY",
        "outputId": "d6de63b1-4c37-40bb-ed5a-dacd36ab08a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'clf__C': 10, 'clf__solver': 'liblinear', 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1)}\n",
            "Best score: 0.7203569903717029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "\n",
        "classify('microsoft/codebert-base')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "ihfq0OzYIBmm",
        "outputId": "d1a99971-2b1d-46c3-c0cb-590a795d2622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-38-7f193124764b>:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1810' max='1938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1810/1938 53:44 < 03:48, 0.56 it/s, Epoch 2.80/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.353800</td>\n",
              "      <td>0.345509</td>\n",
              "      <td>0.910500</td>\n",
              "      <td>0.763562</td>\n",
              "      <td>0.802151</td>\n",
              "      <td>0.728516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.238200</td>\n",
              "      <td>0.227880</td>\n",
              "      <td>0.919411</td>\n",
              "      <td>0.804511</td>\n",
              "      <td>0.775362</td>\n",
              "      <td>0.835938</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-7f193124764b>:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n",
            "<ipython-input-38-7f193124764b>:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n",
            "<ipython-input-38-7f193124764b>:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n",
            "<ipython-input-38-7f193124764b>:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n",
            "<ipython-input-38-7f193124764b>:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "здесь произошел дисконнект коллаба......................................."
      ],
      "metadata": {
        "id": "7Ih448FP7P5j"
      }
    }
  ]
}